{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy your first model in the cloud using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While many data science experiments start with exploring data and training and evaluating a first model on a developer machine. However at some point these models need to be operationalized to actually provide business value.\n",
    "This is where Amazon SageMaker can help which allows to fully automate the whole machine learning lifecycle.\n",
    "\n",
    "Similar to the previous lab we will now train our first model. However we will do the training on infrastructure running in AWS using a simple API call and then create an endpoint which can be used to serve predictions. We will use the Amazon SageMaker built-in XGBoost support to train and serve our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sagemaker      # to work with sagemaker\n",
    "import boto3          # AWS SDK e.g. to uplaod to S3\n",
    "import os\n",
    "\n",
    "# let's use the sagemaker default bucket to store data\n",
    "bucket=sagemaker.Session().default_bucket()\n",
    "# the path in the s3 bucket whre to store artifacts\n",
    "prefix = \"marketing-xgboost\"\n",
    "# get the default sagemaker role which will run the training job\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker XGBoost can train on data in either as CSV or LibSVM format. For this example, weâ€™ll stick with CSV. It should have the predictor variable in the first column - Not have a header row. The data then needs to be uploaded to S3 so it can be used during Training in Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "validation = pd.read_csv(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put y_yes column at the front and write final CSV\n",
    "first_column = train.pop('y_yes')\n",
    "train.insert(0, 'y_yes', first_column)\n",
    "train.to_csv('train_final.csv', index=False, header=False)\n",
    "\n",
    "first_column = validation.pop('y_yes')\n",
    "validation.insert(0, 'y_yes', first_column)\n",
    "validation.to_csv('validation_final.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll copy the file to S3 for Amazon SageMaker's managed training to pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train_final.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to specify the ECR container location for Amazon SageMaker's implementation of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to specify training parameters to the estimator.  This includes:\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data\n",
    "1. Algorithm hyperparameters\n",
    "\n",
    "And then a `.fit()` function which specifies:\n",
    "1. S3 location for output data.  In this case we have both a training and validation set which are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-15 14:20:57 Starting - Starting the training job...\n",
      "2022-06-15 14:21:23 Starting - Preparing the instances for trainingProfilerReport-1655302857: InProgress\n",
      ".........\n",
      "2022-06-15 14:22:56 Downloading - Downloading input data...\n",
      "2022-06-15 14:23:28 Training - Downloading the training image......\n",
      "2022-06-15 14:24:18 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2022-06-15:14:24:21:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2022-06-15:14:24:21:INFO] File size need to be processed in the node: 4.35mb. Available memory size in the node: 8454.51mb\u001b[0m\n",
      "\u001b[34m[2022-06-15:14:24:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[14:24:21] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[14:24:21] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2022-06-15:14:24:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[14:24:21] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[14:24:21] 8279x59 matrix with 488461 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.103014#011validation-error:0.106655\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.102424#011validation-error:0.100012\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.102043#011validation-error:0.099287\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.102078#011validation-error:0.097838\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.102182#011validation-error:0.098683\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.102424#011validation-error:0.098321\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.102633#011validation-error:0.098683\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.102563#011validation-error:0.098079\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.102286#011validation-error:0.097959\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.102251#011validation-error:0.098321\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.1018#011validation-error:0.098442\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.101592#011validation-error:0.098925\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.101211#011validation-error:0.098321\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.101211#011validation-error:0.098442\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.101245#011validation-error:0.099287\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.101315#011validation-error:0.098321\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.101245#011validation-error:0.099287\u001b[0m\n",
      "\u001b[34m[14:24:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.100933#011validation-error:0.099167\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.100864#011validation-error:0.098563\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.101106#011validation-error:0.099287\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.101002#011validation-error:0.09965\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.100551#011validation-error:0.099891\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.100725#011validation-error:0.099167\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.100274#011validation-error:0.098925\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.100066#011validation-error:0.099287\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.09965#011validation-error:0.099529\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.099476#011validation-error:0.099408\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.099754#011validation-error:0.09965\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.099823#011validation-error:0.099408\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.099823#011validation-error:0.09965\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.099372#011validation-error:0.099167\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.099164#011validation-error:0.098925\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.098713#011validation-error:0.099046\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.098713#011validation-error:0.099046\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.098713#011validation-error:0.099167\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.098679#011validation-error:0.099287\u001b[0m\n",
      "\u001b[34m[14:24:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.098609#011validation-error:0.09965\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 20 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.09854#011validation-error:0.099529\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.098366#011validation-error:0.099287\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 24 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.098505#011validation-error:0.099408\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.098262#011validation-error:0.099167\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.098262#011validation-error:0.09965\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.09854#011validation-error:0.099287\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.098366#011validation-error:0.099771\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.098436#011validation-error:0.09965\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.098574#011validation-error:0.099891\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.098609#011validation-error:0.099891\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.098505#011validation-error:0.100012\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.09847#011validation-error:0.100133\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 28 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.098505#011validation-error:0.100374\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.098574#011validation-error:0.100012\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.09847#011validation-error:0.099529\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.09854#011validation-error:0.09965\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.098436#011validation-error:0.09965\u001b[0m\n",
      "\u001b[34m[14:24:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.098436#011validation-error:0.099891\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.098366#011validation-error:0.099771\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.098401#011validation-error:0.100254\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.098436#011validation-error:0.100133\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.09854#011validation-error:0.100254\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 24 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.098609#011validation-error:0.100374\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 30 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.09854#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.098574#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 26 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.098436#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.098297#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 16 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.098332#011validation-error:0.101099\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 36 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.098297#011validation-error:0.10122\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.098228#011validation-error:0.100978\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.098089#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.098193#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.098019#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 14 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.098089#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.097985#011validation-error:0.100374\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.098054#011validation-error:0.100254\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.098054#011validation-error:0.100254\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 26 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.097846#011validation-error:0.100374\u001b[0m\n",
      "\u001b[34m[14:24:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.097846#011validation-error:0.100374\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.097846#011validation-error:0.100374\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.097777#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.097534#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.097638#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.097707#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 34 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.097742#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 24 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.097465#011validation-error:0.100254\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 22 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.097291#011validation-error:0.100133\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.097395#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.09736#011validation-error:0.100374\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.097291#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.097534#011validation-error:0.100978\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.097569#011validation-error:0.100978\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.097291#011validation-error:0.100858\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.097222#011validation-error:0.100374\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.097118#011validation-error:0.100495\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.096979#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 34 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.096979#011validation-error:0.100616\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 30 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.09691#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.09691#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 26 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.096771#011validation-error:0.100858\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.09684#011validation-error:0.100737\u001b[0m\n",
      "\u001b[34m[14:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.09691#011validation-error:0.100616\u001b[0m\n",
      "\u001b[34m[14:24:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.09691#011validation-error:0.100374\u001b[0m\n",
      "\n",
      "2022-06-15 14:24:56 Uploading - Uploading generated training model\n",
      "2022-06-15 14:24:56 Completed - Training job completed\n",
      "Training seconds: 123\n",
      "Billable seconds: 123\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hosting\n",
    "Now that we've trained the `xgboost` algorithm on our data, let's deploy a model that's hosted behind a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# cell 18\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "There are many ways to compare the performance of a machine learning model, but let's start by simply comparing actual to predicted values.  In this case, we're simply predicting whether the customer subscribed to a term deposit (`1`) or not (`0`), which produces a simple confusion matrix.\n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint.  Our data is currently stored as NumPy arrays in memory of our notebook instance.  To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV.\n",
    "\n",
    "*Note: For inference with CSV format, SageMaker XGBoost requires that the data does NOT include the target variable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 19\n",
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07057323, 0.04853175, 0.0980761 , ..., 0.03528015, 0.07282169,\n",
       "       0.02169198])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 20\n",
    "import numpy as np\n",
    "\n",
    "def predict(data, predictor, rows=500 ):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(validation.drop([ 'y_yes'], axis=1).to_numpy(), xgb_predictor)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check our confusion matrix to see how well we predicted versus actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7238</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>686</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions   0.0  1.0\n",
       "actuals               \n",
       "0            7238  145\n",
       "1             686  210"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 21\n",
    "pd.crosstab(index=validation['y_yes'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, of the ~4000 potential customers, we predicted 136 would subscribe and 94 of them actually did.  We also had 389 subscribers who subscribed that we did not predict would.  This is less than desirable, but the model can (and should) be tuned to improve this.  Most importantly, note that with minimal effort, our model produced accuracies similar to those published [here](https://core.ac.uk/download/pdf/55631291.pdf).\n",
    "\n",
    "_Note that because there is some element of randomness in the algorithm's subsample, your results may differ slightly from the text written above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic model Tuning (optional)\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose.\n",
    "For example, suppose that you want to solve a binary classification problem on this marketing dataset. Your goal is to maximize the area under the curve (auc) metric of the algorithm by training an XGBoost Algorithm model. You don't know which values of the eta, alpha, min_child_weight, and max_depth hyperparameters to use to train the best model. To find the best values for these hyperparameters, you can specify ranges of values that Amazon SageMaker hyperparameter tuning searches to find the combination of values that results in the training job that performs the best as measured by the objective metric that you chose. Hyperparameter tuning launches training jobs that use hyperparameter values in the ranges that you specified, and returns the training job with highest auc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 22\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                            'min_child_weight': ContinuousParameter(1, 10),\n",
    "                            'alpha': ContinuousParameter(0, 2),\n",
    "                            'max_depth': IntegerParameter(1, 10)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 23\n",
    "objective_metric_name = 'validation:auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 24\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=20,\n",
    "                            max_parallel_jobs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 25\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 26\n",
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 27\n",
    "# return the best training job name\n",
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 28\n",
    "#  Deploy the best trained or user specified model to an Amazon SageMaker endpoint\n",
    "tuner_predictor = tuner.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 29\n",
    "# Create a serializer\n",
    "tuner_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 30\n",
    "# Predict\n",
    "predictions = predict(test_data.drop(['y_no', 'y_yes'], axis=1).to_numpy(),tuner_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 31\n",
    "# Collect predictions and convert from the CSV output our model provides into a NumPy array\n",
    "pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extensions\n",
    "\n",
    "This example analyzed a relatively small dataset, but utilized Amazon SageMaker features such as distributed, managed training and real-time model hosting, which could easily be applied to much larger problems.  In order to improve predictive accuracy further, we could tweak value we threshold our predictions at to alter the mix of false-positives and false-negatives, or we could explore techniques like hyperparameter tuning.  In a real-world scenario, we would also spend more time engineering features by hand and would likely look for additional datasets to include which contain customer information not available in our initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean-up\n",
    "\n",
    "If you are done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 32\n",
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 33\n",
    "tuner_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
